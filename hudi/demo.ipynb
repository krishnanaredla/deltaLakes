{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableName = \"customers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = \"file:///tmp/customers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, \"old address for 1\",\"durham\", '2020-01-01 02:34:54'),\n",
    "  (1, \"current address for 1\",\"durham\", '2020-01-02 02:27:34'),\n",
    "  (2, \"current address for 2\",\"cary\", '2020-01-01 07:34:53'),\n",
    "  (3, \"current address for 3\",\"chapel hil\", '2020-01-01 04:26:15')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = spark.createDataFrame(data,schema=['customerId','address','city','createdOn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkey = \"customerId\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "definingCol = \"createdOn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataKeys = [\"address\", \"city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashKeyCols = [\"customerId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashDataCols = dataKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(df) :\n",
    "        windowBy = Window.partitionBy(col(pkey)).orderBy(col(definingCol))\n",
    "        df = (\n",
    "            df.withColumn(\n",
    "                \"hashKey\",\n",
    "                sha2(\n",
    "                    concat_ws(\"|\", *map(lambda key_cols: col(key_cols), hashKeyCols)), 256\n",
    "                ),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"hashData\",\n",
    "                sha2(\n",
    "                    concat_ws(\"|\", *map(lambda key_cols: col(key_cols), hashDataCols)), 256\n",
    "                ),\n",
    "            )\n",
    "            .withColumn(\"startDate\", col(definingCol))\n",
    "            .withColumn(\"endDate\", lead(col(definingCol)).over(windowBy))\n",
    "        )\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = processData(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = processData(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psypark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = processData(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hudi_options_upsert = {\n",
    "  'hoodie.table.name': tableName,\n",
    "  'hoodie.datasource.write.recordkey.field': 'hashKey',\n",
    "  'hoodie.datasource.write.table.name': tableName,\n",
    "  'hoodie.datasource.write.operation': 'upsert',\n",
    "  'hoodie.datasource.write.precombine.field': 'createdOn',\n",
    "  'hoodie.upsert.shuffle.parallelism': 2, \n",
    "  'hoodie.insert.shuffle.parallelism': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hudi_options_upsert_multi= {\n",
    "  'hoodie.table.name': tableName,\n",
    "  'hoodie.datasource.write.recordkey.field': 'hashKey,hashData',\n",
    "  'hoodie.datasource.write.table.name': tableName,\n",
    "  'hoodie.datasource.write.operation': 'upsert',\n",
    "  'hoodie.datasource.write.precombine.field': 'createdOn',\n",
    "  'hoodie.upsert.shuffle.parallelism': 2, \n",
    "  'hoodie.insert.shuffle.parallelism': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"hudi\"). \\\n",
    "  options(**hudi_options_upsert). \\\n",
    "  mode(\"overwrite\"). \\\n",
    "  save(basePath+'/upsert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"hudi\"). \\\n",
    "  options(**hudi_options_upsert_multi). \\\n",
    "  mode(\"overwrite\"). \\\n",
    "  save(basePath+'/upsert_multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hudi_options_insert = {\n",
    "  'hoodie.table.name': tableName,\n",
    "  'hoodie.datasource.write.recordkey.field': 'hashKey',\n",
    "  'hoodie.datasource.write.table.name': tableName,\n",
    "  'hoodie.datasource.write.operation': 'insert',\n",
    "  'hoodie.datasource.write.precombine.field': 'createdOn',\n",
    "  'hoodie.upsert.shuffle.parallelism': 2, \n",
    "  'hoodie.insert.shuffle.parallelism': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"hudi\"). \\\n",
    "  options(**hudi_options_insert). \\\n",
    "  mode(\"overwrite\"). \\\n",
    "  save(basePath+'/insert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "up1 = spark. \\\n",
    "  read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  load(basePath + \"/upsert/default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "up1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "up2 = spark. \\\n",
    "  read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  option('hoodie.datasource.query.type','incremental')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "up2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "up2 = spark. \\\n",
    "  read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  option('hoodie.datasource.query.type','incremental')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "up2 = spark. \\\n",
    "  read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  option('hoodie.datasource.query.type','incremental'). \\\n",
    "  load(basePath + \"/upsert/default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "up2 = spark. \\\n",
    "  read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  option('hoodie.datasource.query.type','incremental'). \\\n",
    "  option('hoodie.datasource.read.begin.instanttime','20170901080000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "up2 = spark. \\\n",
    "  read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  option('hoodie.datasource.query.type','incremental'). \\\n",
    "  option('hoodie.datasource.read.begin.instanttime','20170901080000'). \\\n",
    "  load(basePath + \"/upsert/default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "up2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = spark. \\\n",
    "  read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  load(basePath + \"/upsert/insert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = spark. \\\n",
    "  read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  load(basePath + \"/insert/default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "incData = [(1, \"new address for 1\",\"durham\", \"2018-03-03 02:34:65\"),\n",
    "  (3, \"current address for 3 update\",\"apex\", \"2018-04-04 09:32:23\"),    \n",
    "  (4, \"new address for 4\",\"Raleigh\", \"2018-04-04 04:49:22\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc =  spark.createDataFrame(incData,schema=['customerId','address',\"city\",'createdOn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "incDF = processData(inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "incDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "incDF.write.format(\"hudi\"). \\\n",
    "  options(**hudi_options_upsert). \\\n",
    "  mode(\"append\"). \\\n",
    "  save(basePath+'/upsert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "incDF.write.format(\"hudi\"). \\\n",
    "  options(**hudi_options_upsert). \\\n",
    "  mode(\"append\"). \\\n",
    "  save(basePath+'/insert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "up = spark. \\\n",
    "  read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  load(basePath + \"/upsert/default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = spark. \\\n",
    "  read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  load(basePath + \"/insert/default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "up.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
